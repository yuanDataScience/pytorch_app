{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85571e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24e62e9",
   "metadata": {},
   "source": [
    "### What an embedding layer does in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e56fc05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input to the embedding layer (nn.Embedding) is a list of indexes\n",
    "# of the word from the vocabulary list\n",
    "input_index = torch.tensor([4,5, 6, 7], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6962a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings is a map with 500 entries, each corresponding to a word\n",
    "# and its value is a randomly initialized vector of 50 dimension\n",
    "embeddings = nn.Embedding(500, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f693cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when inputing the input_index, the four vectors corresponding to\n",
    "# the four input indices will be retrieved\n",
    "embed_vectors = embeddings(input_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d475dd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for a simple embedding, we just sum the four input vectors and\n",
    "# output the flattened one vector\n",
    "embeds = sum(embeddings(input_index)).view(1, -1)\n",
    "embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0913fb3",
   "metadata": {},
   "source": [
    "### Initialization of embedding vectors\n",
    "* `nn.init.normal_(embedding.weight)` initializes the weights with random values from a normal distribution with a mean of 0 and std of 1\n",
    "* `nn.init.constant_(embedding.weight, 0)`: initialize weights with the specific constant value of 0\n",
    "* `nn.init.xavier_uniform()` and `nn.init.xavier_normal_()` are designed to work well with signmoid and tanh activation functions. They are initialized the weights to values that are close to zero, but not too small\n",
    "* `nn.init.kaiming_uniform_()` and `nn.init.kaiming_normal_()` work well with ReLU and its variants (LeakyReLU, PReLU, RReLU, etc)\n",
    "* can be initialized using pre-trained word vectors such as GloVe or word2vec, which have been trained on large corpora and have been shown to be useful for many NLP tasks. The proecess of using a pre-trained word vector is called Fine-tuning\n",
    "``` python\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    # Load a pre-trained embedding model\n",
    "    pretrained_embeddings = torch.randn(10, 50) # Example only, not actual pre-trained embeddings\n",
    "\n",
    "    # Initialize the embedding layer with the pre-trained embeddings\n",
    "    embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "    # you can also use from_pretrained()  method\n",
    "    embedding_layer = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "\n",
    "    # or load from a pretrained model\n",
    "    glove = torchtext.vocab.GloVe(name='6B', dim=300)\n",
    "    embedding_layer = nn.Embedding.from_pretrained(glove.vectors)\n",
    "```\n",
    "\n",
    "  + you can free the embedding layer from being trained by setting     \n",
    "  `embedding_layer.weight.requiresGrad = False` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ae60ba",
   "metadata": {},
   "source": [
    "### Other parameters\n",
    "* sparse option\n",
    "* `padding_idx`\n",
    "* `max_norm`\n",
    "* `norm_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a6c1056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1929,  0.2584,  0.4394],\n",
       "        [-0.0670,  0.4838,  0.0833],\n",
       "        [ 0.3219,  0.2301,  0.2915],\n",
       "        [ 0.1095,  0.1989,  0.3667]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "# Initialize a sparse matrix: This could be your training set\n",
    "X_train = csr_matrix(np.array([[1, 0, 1, 0],\n",
    "                               [0, 0, 1, 1],\n",
    "                               [1, 1, 1, 0]]))\n",
    "# Get one row: One sample in the training set\n",
    "row = X_train.getrow(0)\n",
    "\n",
    "w_linear = nn.Linear(4, 3, bias=False)\n",
    "w_linear.weight\n",
    "\n",
    "w_embedding = nn.Embedding(4, 3).from_pretrained(w_linear.weight.T)\n",
    "w_embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97be8bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1929, 0.2584, 0.4394],\n",
       "        [0.3219, 0.2301, 0.2915]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.indices\n",
    "w_embedding(torch.tensor(row.indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a951d",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "``` python\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    class Transformer(nn.Module):\n",
    "        def __init__(self, vocab_size, d_model, nhead, num_layers):\n",
    "            super(Transformer, self).__init__()\n",
    "            # This is our holy embedding layer - the topic of this post\n",
    "            self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "            # This is a transformer layer. It contains encoder and decoder\n",
    "            self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
    "\n",
    "            #This is the final fully connected layer that predicts the probability of each word\n",
    "            self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Pass input through the embedding layer\n",
    "            x = self.embedding(x)\n",
    "\n",
    "            # Pass input through the transformer layers (NOTE: This input is usually concatenated with positional encoding. I left it out for simplicity)\n",
    "            x = self.transformer(x)\n",
    "            # Pass input through the final linear layer\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "    # Initialize the model\n",
    "    vocab_size = 10\n",
    "    d_model = 50\n",
    "    nhead = 2\n",
    "    num_layers = 3\n",
    "    model = Transformer(vocab_size, d_model, nhead, num_layers)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69099153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
